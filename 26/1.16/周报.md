# 周报

本周读了论文**SAMWISE:Infusing Wisdom in SAM2 for Text-Driven Video Segmentation**，该工作主要解决了 Meta 最新发布的 **SAM2** 模型在处理**文本指令**和**复杂时序动作**时的局限性。在原本的**SAM2** 只能接受点、框或掩码提示，**SAMWISE** 通过两步策略让它能够理解自然语言，并将其转化为视觉信号。尽管 SAM2 在视觉分割和跟踪上表现强大，但在 RVOS 任务中存在三大硬伤：

1. SAM2 原生仅支持点/框/掩码提示，无法直接处理自然语言描述。
2. SAM2 的特征提取是逐帧独立的，它能跟踪外观相似的物体，但无法理解跨多帧的动作（如区分“正在坐下的人”和“已经坐着的人”）。
3. SAM2 的记忆机制有很强的惯性。一旦初期因为遮挡或歧义跟错了目标（干扰项），即使后续正确目标出现，模型也会因为过度依赖错误记忆而拒绝切换，导致“一步错，步步错”。

这是网络结构图：

![](C:\Users\admin\Desktop\1.16\1.png)

**跨模态时序适配器 (CMT)** 

**机制**：设计了一种轻量级 Adapter 插入到 SAM2 图像编码器的层间。

**时序建模 (HSA)**：引入分层选择性注意力（Hierarchical Selective Attention），将视频特征划分为 3D Patch，让当前帧像素能“看到”前后帧的邻域。这填补了 SAM2 缺失的**动作捕捉能力**。

**早期模态融合**：在特征提取阶段就进行视觉与文本的双向交互（Visual-to-Text / Text-to-Visual Attention），让视觉特征根据文本描述自适应调整（例如听到“红色”时增强红色区域特征）。

**条件记忆编码器 (CME)**

**机制**：引入一个可学习的“判别模块”，实时监控两条特征流：

**记忆流Token ($\tau_l$, Memory-less Token)**：代表模型当前正在跟踪的物体。

**无记忆流Token ($\tau_l$, Memory-less Token)**：代表当前帧中最符合文本描述的物体（不看历史）。

**纠错策略**：当 CME 发现“当前最像文本的物体”与“正在跟踪的物体”不一致时，会判定为跟错，并通过强制更新记忆库。这赋予了 SAM2 **自我反思和纠错**的能力，使其能平滑地切换到正确目标。

结果对比：

![](C:\Users\admin\Desktop\1.16\2.png)