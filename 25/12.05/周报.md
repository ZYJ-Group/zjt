# 周报

利用SAM的vit-h对空间目标TG进行分割，对上次分割图像的代码修改后，分割效果比之前好，下面是原图和分割的结果图：

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/原图.png)

分割图如下：

![分割结果](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/分割结果.png)

指标如下：

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/%E6%8C%87%E6%A0%87.png)

对于之前指标过低原因是标注的标签有问题。

#### SAM2(Segment Anything Model2)论文阅读

**Segment Anything model2 (SAM 2)**，这是一个用于视频和图像分割的统一模型。

**1.提示式视觉分割（promptable visual segmentation）**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/pvs.png)

SAM2重点研究了将图像分割推广到视频领域的提示视觉分割(PVS)任务。该任务将视频的任何帧上的输入点、框或掩码作为输入点，以定义要预测其时空掩码(即“掩码”)的感兴趣的片段。一旦预测了掩码，就可以通过在额外的帧中提供提示来迭代地改进它。



**2.SAM2模型**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/Model.png)

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/SAM2.png)

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/Hiera.png)

图像编码器：对于任意长视频的实时处理，我们采用流媒体的方式，在视频帧可用时进行消费。图像编码器在整个交互过程中只运行一次，它的作用是提供代表每一帧的无条件令牌(特征嵌入)。我们使用
MAE 预训练的Hiera图像编码器，它是分层的，允许我们在解码过程中使用多尺度特征。

记忆注意：记忆注意的作用是将当前框架的特征限定在过去框架的特征和预测以及任何新的提示上。我们堆叠L个变换块，第一个将当前帧的图像编码作为输入。每个块执行自注意力机制，然后交叉注意(提示/非提示)帧和对象指针的记忆，存储在内存库中，然后是一个MLP。我们对自我注意和交叉注意使用普通的注意操作，使我们能够从高效注意核的最新发展中受益。

提示编码器和掩码解码器：我们的提示编码器与SAM的相同，可以通过点击(正或负)、框或掩码来提示，以定义给定帧中对象的范围。稀疏提示由位置编码表示，并对每种提示类型的学习嵌入求和，而掩码则使用卷积嵌入并与帧嵌入求和。

记忆编码器：存储器编码器通过使用卷积模块对输出掩码进行下采样，并将其与图像编码器的无条件帧嵌入(未在图中显示)进行元素求和，然后通过轻量级卷积层融合信息来生成存储器。

记忆库：内存库通过维持一个最多N个最近帧的FIFO记忆队列来保留视频中目标对象的过去预测信息，并将提示信息存储在一个最多M个提示帧的FIFO队列中。例如，在VOS任务中，初始掩码是唯一的提示，内存库始终保留第一帧的记忆以及最多N个最近(未提示)帧的记忆。这两组记忆都以空间特征图的形式存储。

解码器：

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/编码器.png)

与SAM1的结构大致相同，多了几处模块，stride 4, 8 feats. from img. enc. 指的是来自Hiera图像编码器的浅层高分辨率特征，其作用是恢复细节。occlusion score（遮挡分数），作用是在视频跟踪中，物体经常会短暂消失。如果模型预测分数显示物体被遮挡了，它就不会强行分割出一个错误的形状，也不会将这一帧的错误信息存入记忆库。`obj ptr` (Object Pointer / 对象指针)，作用是它不包含具体的像素位置信息，但包含了物体的高级语义特征（比如“这只狗的ID特征”）。这个指针会被送回记忆库，作为下一帧检索记忆的key。

**3.数据引擎**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.05/data.png)

与**SAM**一样分成三个阶段：

阶段 1（逐帧 SAM 标注）中，标注者依托交互式 SAM 工具及像素级手动编辑（如画笔与橡皮擦）对视频逐帧进行独立标注，不使用任何时间传播机制。该方式虽能获得高精度的空间掩码，但标注效率较低，平均每帧耗时 37.8 s。在该阶段，我们对 1.4K 段视频共生成了 16K 高质量掩码，并进一步利用此方式完成 SA-V 验证集与测试集的标注，以避免评估中引入模型偏差。

 阶段 2（SAM + SAM 2 Mask）在第一阶段的基础上，引入具备时序传播能力的 SAM 2 Mask。标注者在首帧通过 SAM 获得初始掩码，并由 SAM 2 Mask 将其临时传播至后续帧，形成完整的时空掩码序列；当传播结果不准确时，标注者可在任意帧对掩码进行空间修改并重新传播。SAM 2 Mask 在循环中基于阶段 1 数据与公开数据集进行了两次再训练。该阶段共获得 63.5K 掩码，平均标注时间降至 7.4 s/帧，效率约提升 5.1 倍。
 
 阶段 3（完整的 SAM 2）进一步采用具备统一交互分割与时序传播能力的全功能 SAM 2，其可接收点、掩码等多种提示，并利用跨帧对象记忆显著提升稳定性。标注过程中，注释者仅需偶尔提供细化点击即可修正中间帧的预测掩码，无需像阶段 1 那样从头绘制。SAM 2 在标注循环中累计完成五次再训练。最终，该阶段标注效率提升至 4.5 s/帧（较阶段 1 提升约 8.4 倍），并生成 197.0K 掩码。



