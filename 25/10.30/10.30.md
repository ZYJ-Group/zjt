# **周报**

#### **SAM(Segment Anything)论文阅读**

该论文的目标是图像分割领域构建首个真正意义上的“基础模型”（foundation model）。去实现强大泛化的任务在广泛的数据集上进行预训练。

**1.可提示分割任务（Promptable Segmentation Task）**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-29%20155310.png)

它将分割问题重新定义为对一个灵活“提示”的响应。该任务要求模型根据用户提供的任何形式的输入（如点、框或文本）来生成一个有效的分割掩码 （这⾥的有效指的是即便提供的prompt 模棱两可，也能返回与其相关的各个部分的 mask。例如当prompt点在了衣服上，会返回衣服和穿着衣服的人的掩码 ）。

**2.Segment Anything模型（SAM）**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-29%20155315.png)

SAM 的架构由三个组件组成，它们协同工作以返回有效的分割掩码：用于生成一次性图像嵌入的图像编码器，一个嵌入提示的提示编码器，一种轻量级掩码解码器，它结合了提示符和图像编码器的嵌入，该轻量化解码器进行分割掩码。

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/univercel-segmentation-model.png)

图像编码器：SAM的骨干是一个大型的、经过MAE（Masked Autoencoders）预训练的视觉Transformer（Vision Transformer, ViT-H）。其作用是在接收一张新图像时，仅运行一次，生成一个强大的、高维度的图像嵌入（image embedding）。这个嵌入向量可以被视为对整个图像内容进行表示。下图是MAE的结构图

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-29%20170106.png)

提示编码器：该模块负责将不同类型的用户提示转换为统一的向量嵌入。对于稀疏提示，如点、边界框和文本，点和边界框使用位置编码来表示其空间坐标，并与一个可学习的、代表提示类型（如“前景点”、“左上角”）的嵌入向量相加。对于文本提示，它利用一个现成的、强大的CLIP文本编码器来生成文本嵌入。对于密集提示（如粗糙掩码），它通过几层轻量级的卷积网络进行处理，然后与图像嵌入逐元素相加。

轻量级掩码解码器：

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-29%20233652.png)



**3.数据引擎**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/10.30/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-29%20155318.png)

数据引擎分为三个阶段：

模型辅助人工标注中，专业标注员使用基于浏览器的交互式分割工具（由SAM提供支持）来标注前景/背景点。随着模型的改进，标注过程变得更加高效，每个掩码的平均标注时间显著减少。

半自动模式下，SAM 可以根据可能的物体位置提示，自动为一部分物体生成掩码，而标注员则专注于标注剩余的物体，从而增加掩码的多样性。接下来，重点转向通过检测置信度高的掩码并邀请标注员标注更多物体来增加掩码的多样性。这一阶段显著扩充了数据集，增强了模型的分割能力。


全自动模式下，人工标注员使用规则的前景点网格提示SAM模型，平均每张图像可生成100个高质量掩码。之后，标注过程完全自动化，利用模型增强和技术（例如模糊感知预测和非极大值抑制）大规模生成高质量掩码。
