# 周报

### SAM3（Segment Anything Model3)论文阅读

SAM 3作为分割基础模型的第三代迭代，实现了从“视觉提示”到“概念提示”的根本性跨越。

**1.提示性概念分割 (Promptable Concept Segmentation)**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.12/提示概念分割（psc）.png)

简而言之，PCS 的核心目标是让模型理解用户所指定的“概念”，并在整幅图像中找出所有符合该概念的实例，而不仅是分割某一个特定目标。用户首先通过文本提示与正向示例框共同定义概念，模型据此生成初步分割结果；随后用户通过新增的正负示例进一步纠正概念边界，使模型区分哪些属于目标概念、哪些不属于。经过这一交互式细化过程，模型最终能够准确识别并分割图中所有符合语义概念的对象。这一机制体现了 SAM 3 从传统的局部点选式分割向基于概念理解的全局语义检索能力的跃迁。

**2.SAM3模型**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.12/SAM3-model.png)

检测器架构：检测器的体系结构遵循一般的DETR范式。图像和文本提示符首先由PE编码，并且图像示
例(如果存在)由示例编码器编码。我们将图像范例令牌和文本令牌联合称为“提示令牌”。然后，融合
编码器接受来自图像编码器的无条件嵌入，并通过交叉关注提示令牌来对它们进行条件化。融合之后是一个类似于DETR的解码器，其中学习到的对象查询交叉关注来自融合编码器的条件图像嵌入。

跟踪器和视频架构：给定一个视频和一个提示P，我们使用检测器和跟踪器(见图)在整个视频中检测和跟踪与提示对应的对象。在每一帧上，检测器找到新的对象Ot时刻t−1的帧传播到当前时刻t上它们的新位置M t。我们使用匹配函数将传播的掩码M t与当前帧中出现的新对象掩码Ot,关联起来跟踪模块通过类似于SAM 2中的视频对象分割任务的单帧传播步骤，根据已经跟踪的对象的先前位置Mt−1预测新的掩码位置Mt，并且跟踪器将掩码Mt−1(时空掩码)从前一。跟踪器与检测器共享相同的图像/帧编码器(PE骨干)。在训练检测器之后，我们冻结PE并像SAM 2一样训练跟踪器，包括提示编码器，掩码解码器，记忆编码器和编码的记忆使用来自过去帧和条件帧(对象首次被检测或用户提示的帧)的特征的对象外观。所述记忆编码器是具有跨当前框架上的视觉特征的自注意力机制和从视觉特征到记忆库中的空间记忆特征的交叉注意的变压器。

检测器结构：

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.12/SAM3model.png)



**3.数据引擎**

![](https://github.com/ZYJ-Group/zjt/blob/main/25/12.12/数据引擎.png)

与SAM1和SAM2一样分成三个阶段：

阶段一人工验证：初始阶段采用完全人工主导的验证流程，以解决模型训练初期的数据冷启动问题。系统利用基础模型生成名词短语并结合 SAM 2 生成初始掩码，随后由人工对掩码的质量与完整性进行全面审核和修正，从而产出约 430 万条高精度图像—文本对，构成 SAM 3 的核心基础数据。

阶段二人机协作验证：在获得高质量基础数据后，系统引入人机协作模式，通过使用第一阶段的数据微调 Llama 3.2 作为 AI 验证者，使其自动完成大部分掩码审核工作，人类标注员仅在困难样本上提供最终确认。与此同时，本阶段加入硬负样本挖掘机制，以增强模型在易混淆概念上的判别能力，并显著提升整体数据生产效率。


阶段三规模化与领域扩展：在模型能力成熟后，系统进入完全自动化的规模化生产阶段，重点扩展长尾语义和复杂场景的覆盖。借助“SAM 3 + AI 验证者”闭环，模型能够主动挖掘数据缺口并自动生成超过 14 亿个高可信度掩码，形成 SA-Co/SYN 大规模合成数据集，用于进一步提升模型的泛化性与跨域适应能力。

