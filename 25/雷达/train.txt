"""
SAM 多类别分割训练脚本
训练前 70 张图片，支持 cabin 和 solar array 两个部件
"""
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np
from datetime import datetime
from PIL import Image, ImageDraw
import json

# 添加父目录到路径
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from segment_anything import sam_model_registry
from config import Config

NUM_CLASSES = 3  # 背景 + cabin + solar array

# ----------------------------
# 标签 JSON 转 mask
# ----------------------------
def json_to_mask(json_file, img_size=(512, 512)):
    """把 JSON 标签转换为多类别 mask (HxW)"""
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    mask = np.zeros(img_size, dtype=np.uint8)
    
    for obj in data['objects']:
        category = obj['category']
        poly = [tuple(pt) for pt in obj['segmentation']]
        img = Image.new('L', img_size, 0)
        ImageDraw.Draw(img).polygon(poly, outline=0, fill=1)
        poly_mask = np.array(img)
        
        if category == 'cabin':
            mask[poly_mask==1] = 1
        elif category == 'solar array':
            mask[poly_mask==1] = 2
    
    return mask

# ----------------------------
# Dataset
# ----------------------------
from torch.utils.data import Dataset, DataLoader

class SegDataset(Dataset):
    def __init__(self, img_dir, mask_dir, img_list, transform=None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.img_list = img_list
        self.transform = transform
    
    def __len__(self):
        return len(self.img_list)
    
    def __getitem__(self, idx):
        img_name = self.img_list[idx]
        img_path = os.path.join(self.img_dir, img_name)
        mask_path = os.path.join(self.mask_dir, img_name.replace('.bmp', '.json'))
        
        # 图像
        img = Image.open(img_path).convert('RGB')
        img = np.array(img).astype(np.float32)/255.0
        img = torch.from_numpy(img).permute(2,0,1)  # C,H,W
        
        # mask
        mask = json_to_mask(mask_path, img_size=(img.shape[1], img.shape[2]))
        mask = torch.from_numpy(mask).long()
        
        return {'image': img, 'mask': mask}

# ----------------------------
# SAM 微调器
# ----------------------------
class SAMFineTuner:
    """SAM模型微调器，支持多类别"""
    
    def __init__(self, model_type='vit_b', checkpoint_path=None, device='cuda'):
        self.device = device
        self.model_type = model_type
        
        print(f"加载SAM模型: {model_type}")
        self.model = sam_model_registry[model_type](checkpoint=checkpoint_path)
        self.model.to(device)
        
        # 冻结图像编码器
        for param in self.model.image_encoder.parameters():
            param.requires_grad = False
        
        # 解冻 mask decoder
        for param in self.model.mask_decoder.parameters():
            param.requires_grad = True
        
        # 损失函数
        self.criterion_ce = nn.CrossEntropyLoss()
        
        # 优化器
        self.optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=1e-4,
            weight_decay=1e-4
        )
        
        # 学习率调度器
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
    
    # Dice loss 多类别
    def dice_loss(self, pred, target, smooth=1.0):
        pred = torch.softmax(pred, dim=1)
        loss = 0
        for c in range(1, NUM_CLASSES):  # 背景可跳过
            pred_c = pred[:, c, :, :]
            target_c = (target==c).float()
            intersection = (pred_c * target_c).sum(dim=(1,2))
            union = pred_c.sum(dim=(1,2)) + target_c.sum(dim=(1,2))
            dice = (2*intersection + smooth) / (union + smooth)
            loss += 1 - dice.mean()
        return loss
    
    # IoU 多类别
    def compute_iou(self, pred, target):
        pred = torch.argmax(pred, dim=1)
        ious = []
        for c in range(1, NUM_CLASSES):
            inter = ((pred==c) & (target==c)).sum().float()
            union = ((pred==c) | (target==c)).sum().float()
            iou = (inter/union).item() if union>0 else 1.0
            ious.append(iou)
        return ious
    
    # 训练单个 epoch
    def train_epoch(self, train_loader, epoch):
        self.model.train()
        total_loss = 0
        total_iou = [0]* (NUM_CLASSES-1)
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
        for batch in pbar:
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            B = images.shape[0]
            
            batch_loss = 0
            batch_iou = [0]* (NUM_CLASSES-1)
            
            for i in range(B):
                image = images[i:i+1]
                mask = masks[i:i+1]
                
                # 图像编码
                with torch.no_grad():
                    image_embeddings = self.model.image_encoder(image)
                
                _, _, H, W = image.shape
                points = torch.tensor([[[W//2, H//2]]], dtype=torch.float32).to(self.device)
                labels = torch.ones((1,1), dtype=torch.int).to(self.device)
                
                # 预测 mask
                sparse_emb, dense_emb = self.model.prompt_encoder(
                    points=(points, labels), boxes=None, masks=None
                )
                low_res_masks, _ = self.model.mask_decoder(
                    image_embeddings=image_embeddings,
                    image_pe=self.model.prompt_encoder.get_dense_pe(),
                    sparse_prompt_embeddings=sparse_emb,
                    dense_prompt_embeddings=dense_emb,
                    multimask_output=False
                )
                
                masks_pred = torch.nn.functional.interpolate(
                    low_res_masks, size=(H, W), mode='bilinear', align_corners=False
                )
                
                # 转换为 3 通道 logits
                background = 1 - masks_pred.sum(dim=1, keepdim=True)
                masks_pred_3ch = torch.cat([background, masks_pred], dim=1)
                
                # 损失
                loss_ce = self.criterion_ce(masks_pred_3ch, mask)
                loss_dice = self.dice_loss(masks_pred_3ch, mask)
                loss = loss_ce + loss_dice
                
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                iou = self.compute_iou(masks_pred_3ch, mask)
                
                batch_loss += loss.item()
                for j in range(NUM_CLASSES-1):
                    batch_iou[j] += iou[j]
            
            total_loss += batch_loss / B
            for j in range(NUM_CLASSES-1):
                total_iou[j] += batch_iou[j] / B
            
            pbar.set_postfix({
                'loss': f'{batch_loss/B:.4f}',
                'IoU_cabin': f'{total_iou[0]/len(train_loader):.4f}',
                'IoU_solar': f'{total_iou[1]/len(train_loader):.4f}'
            })
        
        avg_iou = [x/len(train_loader) for x in total_iou]
        return total_loss/len(train_loader), avg_iou
    
    # 验证函数
    def validate(self, val_loader):
        self.model.eval()
        total_loss = 0
        total_iou = [0]*(NUM_CLASSES-1)
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc='Validation'):
                images = batch['image'].to(self.device)
                masks = batch['mask'].to(self.device)
                B = images.shape[0]
                
                batch_loss = 0
                batch_iou = [0]*(NUM_CLASSES-1)
                
                for i in range(B):
                    image = images[i:i+1]
                    mask = masks[i:i+1]
                    
                    image_embeddings = self.model.image_encoder(image)
                    
                    _, _, H, W = image.shape
                    points = torch.tensor([[[W//2, H//2]]], dtype=torch.float32).to(self.device)
                    labels = torch.ones((1,1), dtype=torch.int).to(self.device)
                    
                    sparse_emb, dense_emb = self.model.prompt_encoder(
                        points=(points, labels), boxes=None, masks=None
                    )
                    low_res_masks, _ = self.model.mask_decoder(
                        image_embeddings=image_embeddings,
                        image_pe=self.model.prompt_encoder.get_dense_pe(),
                        sparse_prompt_embeddings=sparse_emb,
                        dense_prompt_embeddings=dense_emb,
                        multimask_output=False
                    )
                    
                    masks_pred = torch.nn.functional.interpolate(
                        low_res_masks, size=(H, W), mode='bilinear', align_corners=False
                    )
                    background = 1 - masks_pred.sum(dim=1, keepdim=True)
                    masks_pred_3ch = torch.cat([background, masks_pred], dim=1)
                    
                    loss_ce = self.criterion_ce(masks_pred_3ch, mask)
                    loss_dice = self.dice_loss(masks_pred_3ch, mask)
                    loss = loss_ce + loss_dice
                    
                    iou = self.compute_iou(masks_pred_3ch, mask)
                    
                    batch_loss += loss.item()
                    for j in range(NUM_CLASSES-1):
                        batch_iou[j] += iou[j]
                
                total_loss += batch_loss / B
                for j in range(NUM_CLASSES-1):
                    total_iou[j] += batch_iou[j] / B
        
        avg_iou = [x/len(val_loader) for x in total_iou]
        return total_loss/len(val_loader), avg_iou
    
    # 保存 checkpoint
    def save_checkpoint(self, epoch, loss, save_path):
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
        }, save_path)
        print(f"检查点已保存: {save_path}")


# ----------------------------
#
